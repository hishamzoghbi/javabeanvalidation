<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Improve multicore scaling in Open vSwitch DPDK</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/19/improve-multicore-scaling-open-vswitch-dpdk" /><author><name>Kevin Traynor</name></author><id>d4750d8d-a50c-4633-9329-b7a4e8d905ac</id><updated>2021-11-19T07:00:00Z</updated><published>2021-11-19T07:00:00Z</published><summary type="html">&lt;p&gt;A new feature in version 2.16 of &lt;a href="https://docs.openvswitch.org/"&gt;Open vSwitch (OVS)&lt;/a&gt; helps developers scale the &lt;a href="https://docs.openvswitch.org/en/latest/intro/install/dpdk/"&gt;OVS-DPDK&lt;/a&gt; userspace datapath to use multiple cores. The &lt;a href="https://www.dpdk.org"&gt;Data Plane Development Kit (DPDK)&lt;/a&gt; is a popular set of networking libraries and drivers that provide fast packet processing and I/O.&lt;/p&gt; &lt;p&gt;After reading this article, you will understand the new &lt;code&gt;group&lt;/code&gt; assignment type for spreading the datapath workload across multiple cores, how this type differs from the default &lt;code&gt;cycles&lt;/code&gt; assignment type, and how to use the new type in conjunction with the auto load balance feature in the poll mode driver (PMD) to improve OVS-DPDK scaling.&lt;/p&gt; &lt;h2 id="background"&gt;How PMD threads manage packets in the userspace datapath&lt;/h2&gt; &lt;p&gt;In the OVS-DPDK userspace datapath, receive queues (RxQs) store packets from an interface that need to be received, processed, and usually transmitted to another interface. This work is done in OVS-DPDK by PMD threads that run on a set of dedicated cores and that continually poll the RxQs for packets. In OVS-DPDK, these datapath cores are commonly referred to just as PMDs.&lt;/p&gt; &lt;p&gt;When there's more than one PMD, the workload should ideally be spread equally across them all. This prevents packet loss in cases where some PMDs may be overloaded while others have no work to do. In order to spread the workload across the PMDs, the interface RxQs that provide the packets need to be carefully assigned to the PMDs.&lt;/p&gt; &lt;p&gt;The user can manually assign individual RxQs to PMDs with the &lt;code&gt;other_config:pmd-rxq-affinity&lt;/code&gt; option. By default, OVS-DPDK also automatically assigns them. In this article, we focus on OVS-DPDK's process for automatically assigning RxQs to PMDs.&lt;/p&gt; &lt;h3 id="ovs-dpdk-automatic-assignment"&gt;OVS-DPDK automatic assignment&lt;/h3&gt; &lt;p&gt;RxQs can be automatically assigned to PMDs when there is a reconfiguration, such as the addition or removal of either RxQs or PMDs. Automatic assignment also occurs if triggered by the PMD auto load balance feature or the &lt;code&gt;ovs-appctl dpif-netdev/pmd-rxq-rebalance&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;The default &lt;code&gt;cycles&lt;/code&gt; assignment type assigns the RxQs requiring the most processing cycles to different PMDs. However, the assignment also places the same or similar number of RxQs on each PMD.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;cycles&lt;/code&gt; assignment type is a trade-off between optimizing for the current workload and having the RxQs spread out across PMDs to mitigate against workload changes. The default type is designed this way because, when it was introduced in OVS 2.9, there was no PMD auto load balance feature to deal with workload changes.&lt;/p&gt; &lt;h3 id="the-role-of-pmd-auto-load-balance"&gt;The role of PMD auto load balance&lt;/h3&gt; &lt;p&gt;PMD auto load balance is an OVS-DPDK feature that dynamically detects an imbalance created by the user in how the workload is spread across PMDs. If PMD auto load balance estimates that the workload can and should be spread more evenly, it triggers an RxQ-to-PMD reassignment. The reassignment, and the ability to rebalance the workload evenly among PMDs, depends on the RxQ-to-PMD assignment type.&lt;/p&gt; &lt;p&gt;PMD auto load balance is discussed in more detail in another &lt;a href="https://developers.redhat.com/blog/2021/04/29/automatic-load-balancing-for-pmd-threads-in-open-vswitch-with-dpdk"&gt;article&lt;/a&gt;.&lt;/p&gt; &lt;h2 id="the-group-rxq-to-pmd-assignment-type"&gt;The group RxQ-to-PMD assignment type&lt;/h2&gt; &lt;p&gt;In OVS 2.16, the &lt;code&gt;cycles&lt;/code&gt; assignment type is still the default, but a more optimized &lt;code&gt;group&lt;/code&gt; assignment type was added.&lt;/p&gt; &lt;p&gt;The main differences between these assignment types is that the &lt;code&gt;group&lt;/code&gt; assignment type removes the trade-off of having similar numbers of RxQs on each PMD. Instead, this assignment type spreads the workload purely based on finding the best current balance of the workload across PMDs. This improved optimization is feasible now because the PMD auto load balance feature is available to deal with possible workload changes.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;group&lt;/code&gt; assignment type also scales better, because it recomputes the estimated workload on each PMD before every RxQ assignment.&lt;/p&gt; &lt;p&gt;The increased optimization can mean a more equally distributed workload and hence more equally distributed available capacity across the PMDs. This improvement, along with PMD auto load balance, can mitigate against changes in workload caused by changes in traffic profiles.&lt;/p&gt; &lt;h3 id="an-rxq-to-pmd-assignment-comparison"&gt;An RxQ-to-PMD assignment comparison&lt;/h3&gt; &lt;p&gt;We can see some of the differing characteristics of &lt;code&gt;cycles&lt;/code&gt; and &lt;code&gt;group&lt;/code&gt; with an example. If we run OVS 2.16 with a couple RxQs and PMDs, we can check the log messages to confirm that the default &lt;code&gt;cycles&lt;/code&gt; assignment type is used for assigning Rxqs to PMDs:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;|dpif_netdev|INFO|Performing pmd to rx queue assignment using cycles algorithm.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then we can take a look at the current RxQ PMD assignments and RxQ workload usage:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ovs-appctl dpif-netdev/pmd-rxq-show&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;pmd thread numa_id 0 core_id 8: isolated : false port: vhost1 queue-id: 0 (enabled) pmd usage: 20 % port: dpdk0 queue-id: 0 (enabled) pmd usage: 70 % overhead: 0 % pmd thread numa_id 0 core_id 10: isolated : false port: vhost0 queue-id: 0 (enabled) pmd usage: 20 % port: dpdk1 queue-id: 0 (enabled) pmd usage: 30 % overhead: 0 %&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The workload is visualized in Figure 1.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="cycles RxQ-to-PMD assignment" data-entity-type="file" data-entity-uuid="e05e1ca4-49b2-45aa-9cee-c21108884027" src="https://developers.redhat.com/sites/default/files/inline-images/cycles.jpg" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1.Â cycles RxQ-to-PMD assignment.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The display shows that the &lt;code&gt;cycles&lt;/code&gt; assignment type has done a good job keeping the two RxQs that require the most cycles (&lt;code&gt;dpdk0&lt;/code&gt; 70% and &lt;code&gt;dpdk1&lt;/code&gt; 30%) on different PMDs. Otherwise, one PMD would be at 100% and Rx packets might be dropped as a result.&lt;/p&gt; &lt;p&gt;The display also shows that the assignment insists on both PMDs having an equal number of RxQs, two each. This means that PMD 8 is 90% loaded while PMD 10 is 50% loaded.&lt;/p&gt; &lt;p&gt;That is not a problem with the current traffic profile, because both PMDs have enough processing cycles to handle the load. However, it does mean that PMD 8 has available capacity of only 10% to account for any traffic profile changes that require more processing. If, for example, the &lt;code&gt;dpdk0&lt;/code&gt; traffic profile changed and the required workload increased by more than 10%, PMD 8 would be overloaded and packets would be dropped.&lt;/p&gt; &lt;p&gt;Now we can look at how the &lt;code&gt;group&lt;/code&gt; assignment type optimizes for this kind of scenario. First we enable the &lt;code&gt;group&lt;/code&gt; assignment type:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ovs-vsctl set Open_vSwitch . other_config:pmd-rxq-assign=group&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The logs confirm that is selected and immediately put to use:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;|dpif_netdev|INFO|Rxq to PMD assignment mode changed to: `group`.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As mentioned earlier, the &lt;code&gt;group&lt;/code&gt; assignment type eliminates the requirement of keeping the same number of RxQs per PMD, and bases its assignments on estimates of the least loaded PMD before every RxQ assignment. We can see how this policy affects the assignments:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ovs-appctl dpif-netdev/pmd-rxq-show&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;pmd thread numa_id 0 core_id 8: isolated : false port: dpdk0 queue-id: 0 (enabled) pmd usage: 70 % overhead: 0 % pmd thread numa_id 0 core_id 10: isolated : false port: vhost0 queue-id: 0 (enabled) pmd usage: 20 % port: dpdk1 queue-id: 0 (enabled) pmd usage: 30 % port: vhost1 queue-id: 0 (enabled) pmd usage: 20 % overhead: 0 %&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The workload is visualized in Figure 2.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="group RxQ-to-PMD assignment type" data-entity-type="file" data-entity-uuid="62c09dcf-5452-4a90-be7e-df9bc0d1e139" src="https://developers.redhat.com/sites/default/files/inline-images/group.jpg" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2.Â group RxQ-to-PMD assignment type.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now PMD 8 and PMD 10 both have total loads of 70%, so the workload is better balanced between the PMDs.&lt;/p&gt; &lt;p&gt;In this case, if the &lt;code&gt;dpdk0&lt;/code&gt; traffic profile changes and the required workload increases by 10%, it could be handled by PMD 8 without any packet drops because there is 30% available capacity.&lt;/p&gt; &lt;p&gt;An interesting case is where RxQs are new or have no measured workload. If they were all put on the least loaded PMD, that PMD's estimated workload would not change. It would keep being selected as the least loaded PMD and be assigned all the new RxQs. This might not be ideal if those RxQs later became active, so instead the &lt;code&gt;group&lt;/code&gt; assignment type spread RxQs with no measured history among PMDs.&lt;/p&gt; &lt;p&gt;This example shows a change from the &lt;code&gt;cycles&lt;/code&gt; to &lt;code&gt;group&lt;/code&gt; assignment type during operation. Although that can be done,Â  the assignment type is typically set when initializing OVS-DPDK. Reassignments can then be triggered by any of the following mechanisms:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;PMD auto load balance (providing that user-defined thresholds are met)&lt;/li&gt; &lt;li&gt;A change in configuration (adding or removing RxQs or PMDs)&lt;/li&gt; &lt;li&gt;The &lt;code&gt;ovs-appctl dpif-netdev/pmd-rxq-rebalance&lt;/code&gt; command&lt;/li&gt; &lt;/ul&gt;&lt;h2 id="other-rxq-considerations"&gt;Other RxQ considerations&lt;/h2&gt; &lt;p&gt;All of the OVS-DPDK assignment types are constrained by the granularity of the workload on each RxQ. In the example in the previous section, it was possible to spread the workload evenly. In a case where &lt;code&gt;dpdk0&lt;/code&gt; was 95% loaded instead, PMD 8 would have a 95% load, while PMD 10 would have a 70% load.&lt;/p&gt; &lt;p&gt;If you expect an interface to have a high traffic rate and hence a high required load, it is worth considering the addition of more RxQs in order to help split the traffic for that interface. More RxQs mean a greater granularity to help OVS-DPDK spread the workload more evenly across the PMDs.&lt;/p&gt; &lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article looked at the new &lt;code&gt;group&lt;/code&gt; assignment type from OVS 2.16 for RxQ-to-PMD assignments.&lt;/p&gt; &lt;p&gt;Although the existing &lt;code&gt;cycles&lt;/code&gt; assignment type might be good enough in many cases, the new &lt;code&gt;group&lt;/code&gt; assignment type allows OVS-DPDK to more evenly distribute the workload across the available PMDs.&lt;/p&gt; &lt;p&gt;This dynamic assignment has the benefit of allowing more optimal use of PMDs and providing a more equally distributed available capacity across PMDs, which in turn can make them more resilient against workload changes. For larger changes in workload, the PMD auto load balance feature can trigger reassignments.&lt;/p&gt; &lt;p&gt;OVS 2.16 still has the same defaults as OVS 2.15, so users for whom OVS 2.15 multicore scaling is good enough can continue to use it by default after an upgrade. However, the new option is available if required.&lt;/p&gt; &lt;p&gt;Further information about OVS-DPDK PMDs can be found in the &lt;a href="https://docs.openvswitch.org/en/latest/topics/dpdk/pmd/"&gt;documentation&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/19/improve-multicore-scaling-open-vswitch-dpdk" title="Improve multicore scaling in Open vSwitch DPDK"&gt;Improve multicore scaling in Open vSwitch DPDK&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Kevin Traynor</dc:creator><dc:date>2021-11-19T07:00:00Z</dc:date></entry><entry><title type="html">Kogito Tooling Released! 10k+ installs on BPMN extension, Dashbuilder Runtime in Quarkus, and an outstanding KIE Live next week!</title><link rel="alternate" href="https://blog.kie.org/2021/11/kogito-tooling-released-10k-installs-on-bpmn-extension-dashbuilder-runtime-in-quarkus-and-an-outstanding-kie-live-next-week.html" /><author><name>Eder Ignatowicz</name></author><id>https://blog.kie.org/2021/11/kogito-tooling-released-10k-installs-on-bpmn-extension-dashbuilder-runtime-in-quarkus-and-an-outstanding-kie-live-next-week.html</id><updated>2021-11-19T05:00:00Z</updated><content type="html">We have just launched a fresh new Kogito Tooling release! &#x1f389; On the 0.14.1 , we made a lot of improvements and bug fixes. This post will give a quick overview of our most recent . I hope you enjoy it! DONâT MISS THE KIE LIVE NEXT WEEK Our beloved .new environment will receive a massive update! Dealing with complex models and collaborating with others will become much easier. Join us on the 23rd of November in the next for a walkthrough of the new features and new integrations we have with the Developersâ most beloved tools. 10K+ USERS OF VS CODE BPMN EXTENSION Our just reached an important milestone on VS Code Store: 10k+ individual installs! Congrats to Roger (tech lead) and all the BPMN/Stunner team. AUTOMATICALLY GENERATE BPMN/DMN SVG ON VS CODE To provide better integration with the KIE server and Business Central, on the Kogito Tooling 0.14 release, we introduced a way to, on VS Code, automatically generate SVG on each save of your BPMN and DMN Diagram. Take a look at this feature in action: Please take a look at this for further details! DASHBUILDER RUNTIME RELEASED ON KOGITO TOOLING We are glad to announce that we are releasing ! The major change for this new release is the adoption of Quarkus as the backend for DashBuilder Runtime and the introduction of DashBuilder Authoring, a new tool to create dashboards. From now on, we will also follow the Kogito Tooling release cadence! Soon we will publish a blog post with more details! CANVAS API We also just released the first iteration of a for node manipulation in our editors. This new API allows to manipulate the shapes in the canvas, so third parties can play with the different objects in the canvas VISUALIZE, EDIT, AND SHARE YOUR BPMN, DMN, AND PMML WITH GITHUB.DEV Some weeks ago, GitHub released github.dev which allows you to open any repository in VS Code directly from your browser just pressing . (dot key) on it. On Kogito Tooling 0.13.0 release, we updated our VS Code BPMN, DMN, and PMML extension to also work on this innovative environment. Check it out: Please take a look at this for further details! NEW FEATURES, FIXED ISSUES, AND IMPROVEMENTS * â Editor content sanitization * â [DMN Designer] User changes are lost * â Generate a SVG diagram automatically on each BPMN/DMN diagrams save * â [DMN Designer] Improve BKM description rendering on documentation tab * â Stunner â Create an initial JS / TS API for accessing the canvas and its elements * â ScoreCard: MiningField validation * â Mining Schema (PMML Editor test Suite) * â [VSCode] Custom editor save issues * â Editors â Editing the node name and pressing enter to confirm * â [DMN Designer] Unreadable data type information in PDF document that shows DMN decision model * â [DMN Designer] Background color do not work on DMN Editor (online and VSCode) * â [DMN Designer] Multiple DRDs â Renaming a DRD freezes the browser * â Online DMN Editor should support deployment to any Openshift Cluster other than Dev Sandbox * â Collections Data Objects can be filled with expressions only. * â Standalone DMN editor missing isDirty indication on data type or included models change * â Create the second step of the Wizard â Create the collapsible/expandable list of future Data Types * â BPMN Editor â Containment not working when Node overlaps the Connector while splicing * â formInputs should be parsed with dates as objects, not strings. * â Get the Route through the REST API and remove the console URL property. * â Enable extensions for github.dev. * â Activate the DMN dirty indicator test * â DMN Guided tour cypress tests * â Introduce DMN Runner cypress test * â ScoreCard Model Setup Test * â Score Cards: Algorithm Name cannot be cleared * â Score Cards: Data Dictionary: Remove duplication of delete icons * â Metadata atrribute âelementnameâ not present for events, intermediate events &amp;amp; gateways by default * â Filled DateTime field on Dev Sandbox form break Runner when itâs opened on Online Editor * â kogito-examples non unique packages * â kogito-editors-java pre push hooks * â [DMN Designer] New Boxed Expression editor â Remove grip from the new boxed expression editor * â Improvements for the KIE Tooling Extended Services outdated icon * â Fix Quarkus Dev UI DEV mode FURTHER READING/WATCHING We had some excellent blog posts on Kie Blog and Kie Lives that I recommend to you: * , by Paulo; * , by Tiago Dolphine; * , by Pere; * , by Paulo; THANK YOU TO EVERYONE INVOLVED! I would like to thank everyone involved with this release, from the excellent KIE Tooling Engineers to the lifesavers QEs and the UX people that help us look awesome! The post appeared first on .</content><dc:creator>Eder Ignatowicz</dc:creator></entry><entry><title>Design an authorization cache for Envoy proxy using WebAssembly</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/18/design-authorization-cache-envoy-proxy-using-webassembly" /><author><name>Rahul Anand</name></author><id>00c1e42d-6885-4d0a-9491-1dd126cf8007</id><updated>2021-11-18T07:00:00Z</updated><published>2021-11-18T07:00:00Z</published><summary type="html">&lt;p&gt;This article introduces a high-level design to implement an authorization cache associated with the Envoy proxy using WebAssembly. The goal of this project is to reduce the latencies of HTTP requests passing through the Envoy proxy by reducing the traffic to the service responsible for authentication and authorization of requests. The cache stores data about authorization so that the external service needs to be contacted only on cache misses, instead of for every HTTP request.&lt;/p&gt; &lt;p&gt;We also provide the source code of an authorization cache that interacts withÂ &lt;a href="https://developers.redhat.com/products/3scale/overview"&gt;Red Hat 3scale API Management&lt;/a&gt;. The cache was implemented as a part of the &lt;a href="https://summerofcode.withgoogle.com/archive/2021/projects/6551776420954112/"&gt;Google Summer of Code 2021Â project&lt;/a&gt;.Â &lt;/p&gt; &lt;p&gt;This article is the first in a two-part series. This first article introduces a high-level, generic design that will give you a basic idea of the cache's overall functionality. The second part explains the major design decisions and implementation details.&lt;/p&gt; &lt;h2&gt;What is Envoy proxy?&lt;/h2&gt; &lt;p&gt;&lt;a href="https://www.envoyproxy.io/"&gt;Envoy&lt;/a&gt; is an &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;edge&lt;/a&gt; and service proxy for applications running in the cloud. Envoy is valuable for many use cases, including edge proxy, middle proxy, sidecar for service mesh deployments, and a daemon set within &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;. Among Envoy's compelling features, performance, extensibility, and &lt;a href="https://developers.redhat.com/topics/api-management/"&gt;API configurability&lt;/a&gt; are the most prominent, making it unique in the proxy space.Â This article mainly focuses on extensibility.Â &lt;/p&gt; &lt;h2&gt;The Envoy proxy authorization cache in action&lt;/h2&gt; &lt;p&gt;The implementation uses &lt;a href="https://github.com/proxy-wasm/"&gt;Proxy-Wasm&lt;/a&gt;, implementing the extensions as &lt;a&gt;WebAssembly&lt;/a&gt; modules. We made this choice based on the flexibility, portability, maintainability, and isolation (for fault tolerance) that WebAssembly offers when compared with native Envoy filters. At the time of writing this article, Proxy-Wasm supports two types of extensions: Filters and singleton services.&lt;/p&gt; &lt;p&gt;Figure 1 shows the interactions between the proxy, the 3scale Service Management API, and the upstream services when a client sends a request. The request is propagated through the Envoy filter chain. For access to Wasm filters and services, the request interacts with the Wasm Virtual machine (VM) to execute the Wasm plugins.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Higher Level Introductory Diagram" data-entity-type="file" data-entity-uuid="1a15c798-b5ea-40d4-a666-1a11dcdf6969" src="https://developers.redhat.com/sites/default/files/inline-images/Blog%20Diagrams%20-%20Higher%20Level%20Intro_0.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure1.Â Envoy communicates through the Wasm virtual machines with the 3scale Service Management API.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Designing the authorization cache&lt;/h2&gt; &lt;p&gt;In our design, the cache inside the proxy is implemented through two main components: A filter and a singleton service.&lt;/p&gt; &lt;p&gt;The filter is responsible for intercepting HTTP requests, authorizing them based on the stored cache, and performing rate limiting. In the context of the envoy, this component is an HTTP filter and gets executed in the worker threads. For each request, a context object gets created.&lt;/p&gt; &lt;p&gt;The singleton service is responsible for the background synchronization of cached data between the proxy and the 3scale Service Management API. In the context of the envoy, this is a singleton service and gets executed in the main thread outside the request lifecycle. Only one instance of this service gets instantiated in each Envoy process.&lt;/p&gt; &lt;p&gt;Figure 2 shows how the HTTP filter and singleton service interact with the other internal and external components to provide the expected functionality of the in-proxy cache. Shared data is an in-memory key-value store specified by the Proxy-Wasm ABI and provided by the proxy. Each VM contains a shared datastore. Because the two extensions (filter and singleton) are running in the same Wasm VM, both extensions have direct access to the shared data.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/overall.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/overall.png?itok=zqS9QtQJ" width="1440" height="706" alt="The filter's threads and the singleton service share data and a message queue." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. The filter's threads and the singleton service share data and a message queue. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;I/O between the host and VM is done in binary format and thus requires serialization and deserialization. The Proxy-Wasm ABI provides a shared queue that is also unique per VM. In our design, the shared queue is the main communication channel between the singleton service and filter. Any message enqueued is broadcast to all threads, but only one thread can dequeue the message. The singleton service updates the cache records saved in the shared data, either periodically or based on policies defined in the configuration. The HTTP filter uses the cache records in the shared data to perform authorization and rate-limiting.&lt;/p&gt; &lt;h3&gt;Filter design&lt;/h3&gt; &lt;p&gt;The default Envoy proxy makes an external HTTP call to the 3scale Service Management API for every HTTP request to perform authorization and reporting. But with the internal cache, we eliminate this need and limit the external HTTP calls to cache misses (Figure 3). That way, we reduce the traffic on the 3scale Service Management API and therefore overall request latency, to a great extent.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Envoy filter chain with and without cache filter" data-entity-type="file" data-entity-uuid="ecac817d-8571-4ca0-95d6-6012b8f44463" src="https://developers.redhat.com/sites/default/files/inline-images/Blog%20Diagrams%20-%20W%20and%20W_O%20cache_0.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 3. Envoy proxy filter chain with and without cache filter.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The filter serves as the entry point for the request authorization with the help of cached data. The filter takes one of two pathways for each request:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Cache miss:&lt;/strong&gt; This happens when a cache record is not found in the shared data. The filter calls out to the 3scale Service Management API to fetch the latest state.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cache hit:&lt;/strong&gt; This happens when a cache record is present. Based on authorization results, metrics are passed to the singleton for reporting to the 3scale Service Management API.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The filter is equipped with an opt-in feature called &lt;em&gt;unique-callout&lt;/em&gt;, which ensures that there is only one callout to the 3scale Service Management API on a cache miss. This feature increases performance and accuracy under high load, without any impact under low-load conditions. We'll look at the results with and without the unique callout feature in the second part of this series, when we get to examine the benchmarks.&lt;/p&gt; &lt;h3&gt;Singleton design&lt;/h3&gt; &lt;p&gt;The singleton service serves two main functions for the proposed in-proxy cache design:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Collect metrics based on a predefined policy and report them to the external management service.&lt;/li&gt; &lt;li&gt;Update the local cache stored in the proxy by pulling the latest states of applications and services from the external management service.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In our case, the external management service is the 3scale Service Management API.&lt;/p&gt; &lt;p&gt;In Envoy, almost all the functions in worker threads and the main thread get executed in a non-blocking manner. So the methods get invoked as callbacks. In the singleton service, we use two types of events to perform the required functionality:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Periodic events, triggered at predefined intervals via the &lt;code&gt;on_tick()&lt;/code&gt; callback&lt;/li&gt; &lt;li&gt;Events triggered by the filter through the message queue&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Events that are sent from the filter pass through the message queue to the singleton service. The second part of this article offers further details about the integration of cache with the singleton section.&lt;/p&gt; &lt;h2&gt;Coming up: Implementing the cache&lt;/h2&gt; &lt;p&gt;In the next part of this series, we'll dive into the implementation details of the cache, data modeling, various features that made the cache work better, limitations, and planned improvements.Â &lt;/p&gt; &lt;p&gt;In the meantime, here are additional resources to learn more:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;To try this module, check out the &lt;a href="https://github.com/rahulanand16nov/gsoc-wasm-filters/"&gt;project repository on GitHub&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;For a demo using the module integrated with Istio, check out this session presented by Daniel Grimm and Burr Sutter:Â &lt;a href="https://developers.redhat.com/devnation/tech-talks/istio-webassembly"&gt;Hacking the Mesh: Extending Istio with WebAssembly Modules | DevNation Tech Talk&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/18/design-authorization-cache-envoy-proxy-using-webassembly" title="Design an authorization cache for Envoy proxy using WebAssembly"&gt;Design an authorization cache for Envoy proxy using WebAssembly&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Rahul Anand</dc:creator><dc:date>2021-11-18T07:00:00Z</dc:date></entry><entry><title type="html">Edge medical diagnosis - Architectural introduction</title><link rel="alternate" href="http://www.schabell.org/2021/11/edge-medical-diagnosis-architectural-introduction.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2021/11/edge-medical-diagnosis-architectural-introduction.html</id><updated>2021-11-18T06:00:00Z</updated><content type="html">Part 1 - Architectural introduction The last few years we have been digging deeply into the world of architectures with a focus on presenting access to ways of mapping successful implementations for specific use cases. It's an interesting challenge in that we have the mission of creating architectural content based on common customer adoption patterns.Â  That's very different from most of the traditional marketing activities usually associated with generating content for the sole purpose of positioning products for solutions. When you're basing the content on actual execution in solution delivery, you're cutting out theÂ chuff.Â  What's that mean? It means that it's going to provide you with a way to implement a solution using open source technologies by focusing on the integrations, structures and interactions that actually have been proven to work. What's not included are any vendor promises that you'll find in normal marketing content. Those promised that when it gets down to implementation crunch time, might not fully deliver on their promises. Enter the termÂ Portfolio Architecture.Â  Let's look at these architectures, how they're created and what value they provide for your solution designs. THE PROCESS The first step is to decide the use case to start with, which in my case had to be linked to a higher level theme that becomes the leading focus. This higher level theme is not quite boiling the ocean, but it's so broad that it's going to require some division into smaller parts. In this case presented here is we are looking closer at the healthcare industry and we've decided to start with one we're calling the edge medical diagnosis architecture.Â This use case we've defined as the following: Accelerating medical diagnosis using condition detection in medical imagery with AI/ML at medical facilitiesÂ  The approach taken is to research our existing customers that have implemented solutions in this space, collect their public-facing content, research the internal implementation documentation collections from their successful engagements, and where necessary reach out to the field resources involved.Â  To get an idea of what these architectures look like, we refer you to the series previously discussed here: * * * * Now on to the task at hand. WHAT'S NEXT The resulting content for this project targets the following three items. * A slide deck of the architecture for use in telling the portfolio solution story. * Generic architectural diagrams providing the general details for the portfolio solution. * A write-up of the portfolio solution in a series that can be used for a customer solution brief. An overview of this series onÂ edge medical diagnosis architecture: 1. 2. Common architectural elements 3. Example predictive analysis 4. Example architecture with GitOps Catch up on any past articles you missed by following any published links above. Next in this series, we will take a look at theÂ genericÂ common architectural elementsÂ for the edge medical diagnosis architecture.</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title type="html">This Week in JBoss - 18 November 2021</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2021-11-18.html" /><category term="quarkus" /><category term="kogito" /><category term="java" /><category term="resteasy" /><category term="camel" /><category term="reactive" /><author><name>Alex Porcelli</name><uri>https://www.jboss.org/people/alex-porcelli</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2021-11-18.html</id><updated>2021-11-18T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, kogito, java, resteasy, camel, reactive"&gt; &lt;h1&gt;This Week in JBoss - 18 November 2021&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hello! Welcome to another edition of the JBoss Editorial that brings you news and updates from our community.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Here are the releases from the JBoss Community for this edition:&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-4-2-final-released/"&gt;Quarkus 2.4.2&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2021/11/kogito-1-13-0-released.html"&gt;Kogito 1.13.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://camel.apache.org/blog/2021/11/RELEASE-3.13.0/"&gt;Camel 3.13.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://camel.apache.org/blog/2021/11/camel-quarkus-release-2.4.0/"&gt;Camel Quarkus 2.4.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/apache/camel-k/releases/tag/v1.7.0/"&gt;Camel K 1.7.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://in.relation.to/2021/11/09/hibernate-reactive-1_1_0_Final/"&gt;Hibernate Reactive 1.1.0.Final&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://resteasy.github.io/2021/11/04/resteasy-5.0.0-release/"&gt;RESTEasy 5.0.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_test_driven_development_with_quarkus"&gt;Test-driven development with Quarkus&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/08/test-driven-development-quarkus"&gt;Test-driven development with Quarkus&lt;/a&gt;, by Eric Deandrea&lt;/p&gt; &lt;p&gt;Ericâs article provides a detailed walkthrough of how to take advantage of Quarkus continuous testing and Dev UI for test-driven development. Eric also provides a sample repository that you can use to follow along with the article.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_boost_throughput_with_resteasy_reactive_in_quarkus_2"&gt;Boost throughput with RESTEasy Reactive in Quarkus 2.&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/04/boost-throughput-resteasy-reactive-quarkus-22/"&gt;Boost throughput with RESTEasy Reactive in Quarkus 2.&lt;/a&gt;, by Daniel Oh&lt;/p&gt; &lt;p&gt;Daniel shows how some simple changes in your REST endpoints can boost the throughput of your application using RESTEasy Reactive. He also shows how to use the Endpoint score dashboard to assess the performance of your endpoints.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_low_code_camel"&gt;Low Code Camel&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://www.nicolaferraro.me/2021/11/03/low-code-camel/]"&gt;Low Code Camel - How Kamelets enable a low code integration experience.&lt;/a&gt;, by Nicola Ferraro&lt;/p&gt; &lt;p&gt;This excellent post shows how Kamelets is driving a deeper transformation towards "low code" development with Camel. Nicola Ferraro shows that you can use Kamelets directly with yaml or take advantage of the Karavan tool to design and visualize your integration flows graphically.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_getting_started_with_hibernate_reactive_on_quarkus"&gt;Getting started with Hibernate reactive on Quarkus&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/soa-cloud/quarkus/getting-started-with-hibernate-reactive/"&gt;Getting started with Hibernate reactive on Quarkus&lt;/a&gt;, by F. Marchioni&lt;/p&gt; &lt;p&gt;Reactive everywhere! In this post, F. Marchioni helps us get started with Hibernate reactive, providing a step-by-step from project creation to configuration and coding.&lt;/p&gt; &lt;p&gt;The post covers the "classic" Hibernate ORM, but Hibernate Reactive can also be applied to Panache.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_java_lts_perspective_of_a_library_author"&gt;Java LTS - perspective of a library author&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://emmanuelbernard.com/blog/2021/11/15/java-lts/"&gt;Java LTS - perspective of a library author&lt;/a&gt;, by Emmanuel Bernard&lt;/p&gt; &lt;p&gt;Oracle is proposing a change to the Java LTS lifecycle from the current 3 years for 2 years. In this post, Emmanuel Bernard put some light on the challenges, from the perspective of a library author, that such changes may bring to the complex Java ecosystem.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_videos"&gt;Videos&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;YouTube is such a great platform to share knowledge, here are my top picks for this weekâs editorial:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/stH_jA5f5eM"&gt;First Look at Testcontainers Cloud and Quarkus&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/VGAnVX1lCxg"&gt;Quarkus Insights #69: Performance and costs of reactive libraries in Java&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/C5NGczQMHu0"&gt;KIELive #51 TrustyAI: Ensuring the Fairness and Transparency of Decision Models&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;p&gt;&lt;em&gt;Thatâs all for today! Please join us again in two weeks for another round of our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/alex-porcelli.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Alex Porcelli&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Alex Porcelli</dc:creator></entry><entry><title>Managing persistent volume access in Kubernetes</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/17/managing-persistent-volume-access-kubernetes" /><author><name>Don Schenck</name></author><id>338525ef-ccb2-43d7-b529-4639c7dbd9b9</id><updated>2021-11-17T07:00:00Z</updated><published>2021-11-17T07:00:00Z</published><summary type="html">&lt;p&gt;Data storage gets complex in the world of &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;, as we discussed in &lt;a href="https://developers.redhat.com/articles/2021/08/11/how-maximize-data-storage-microservices-and-kubernetes-part-1-introduction"&gt;Part 1 of this series&lt;/a&gt;. That article explained the &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; concept of a &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/"&gt;persistent volume (PV)&lt;/a&gt; and introduced &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift-data-foundation"&gt;Red Hat OpenShift Data Foundation&lt;/a&gt; as a simple way to get persistent storage for your applications running in &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Beyond the question of provisioning storage, one must think about types of storage and access. How will you read and write data? Who needs the data? Where will it be used? Because these questions sound a bit vague, let's jump into some specific examples.&lt;/p&gt; &lt;p&gt;I ran the examples in this article on &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, a free instance of OpenShift that you can use to begin your OpenShift and Kubernetes journey.&lt;/p&gt; &lt;h2&gt;Data on the ROX: ReadOnlyMany&lt;/h2&gt; &lt;p&gt;Read access is the basic right granted by any data source. We'll start with read access and examine writing in a later section.&lt;/p&gt; &lt;p&gt;There are a couple of variants of read access in Kubernetes. If your application reads data from a source but never updates, appends, or deletes anything in the source, the access is &lt;code&gt;ReadOnly&lt;/code&gt;. And, because &lt;code&gt;ReadOnly&lt;/code&gt; access never changes the source, multiple consumers can read from the source at the same time without risk of receiving corrupted results, a type of access called &lt;code&gt;ReadOnlyMany&lt;/code&gt; or ROX.&lt;/p&gt; &lt;p&gt;When a persistent volume claim (PVC) is created, the technology upon which it exists is specified by the storage class used in the YAML configuration file. When you ask for a PVC, you specify a size, a storage class, and an access method such as ROX. It is up to the underlying technology to meet the access method's requirements. And not every technology meets the access requirements of ROX. For example, &lt;a href="https://aws.amazon.com/ebs/"&gt;Amazon Elastic Block Store (EBS)&lt;/a&gt; does not.&lt;/p&gt; &lt;p&gt;If you create a PVC using a storage class that does not support your access method, you do not receive any error message at creation time. It is not until you assign the PVC to an application that any error message appears. For example, through the following configuration, I created a PVC called &lt;code&gt;claim2&lt;/code&gt; that tried to use EBS with a &lt;code&gt;ReadOnlyMany&lt;/code&gt; access level:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;apiVersion: v1 kind: PersistentVolumeClaim metadata: name: claim2 namespace: rhn-engineering-dschenck-dev spec: accessModes: - ReadOnlyMany volumeMode: Filesystem resources: requests: storage: 1Gi&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here's what happened at the command line. OpenShift was happy to oblige me in the creation of this PVC, even though it won't accept data at runtime:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;PS C:\Users\dschenck\Desktop&gt; oc create -f .\persistentvolumeclaim-claim2.yaml persistentvolumeclaim/claim2 created&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Figure 1 shows the claim waiting for assignment.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/pending.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/pending.png?itok=bugMY_7q" width="740" height="282" alt="Openshift allows the PVC to be created." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1. PVC waiting in Pending status.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;I then created an app and assigned this PVC to it. That is when the error appeared, as seen in Figure 2.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/error.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/error.png?itok=VnW_kEXf" width="925" height="104" alt="When the application tries to get access to the PV, an error message is displayed." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. Error message from PV says that the access mode is not supported. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;If the PVC had been based on a storage class that supported ROX, this error would not have appeared.&lt;/p&gt; &lt;p&gt;To avoid crucial runtime problems such as the one I've laid out here, DevOps practices are valuable. Developers and operators work together to identify problems early in the development process.&lt;/p&gt; &lt;h2&gt;ReadWriteOnce (RWO) and ReadWriteMany (RWX)&lt;/h2&gt; &lt;p&gt;Reading and writing data is supported by the &lt;code&gt;ReadWriteOnce&lt;/code&gt; (RWO) and &lt;code&gt;ReadWriteMany&lt;/code&gt; (RWX) access methods. As you can surmise, these methods are dictated by the storage method that underlies the storage class. Regardless of which access method you choose, you must make sure the storage class supports it. Two storage classes that support RWX access are CephFS and the Network File System (NFS).&lt;/p&gt; &lt;p&gt;Creating file system storage and using CephFS is facilitated by theÂ &lt;a href="https://www.redhat.com/en/resources/openshift-data-foundation-overview"&gt;OpenShift Data Foundation (ODF) Operator&lt;/a&gt;, which not only provisions the storage but brings with it a lot of benefits. Snapshots and backups are simple operations with ODF and can be automated as well. If you're going to provision storage, you need to bring along some sort of method for backup and recovery.&lt;/p&gt; &lt;h2&gt;Object storage&lt;/h2&gt; &lt;p&gt;Traditionally, operating systems and storage media have offered file systems and block storage. One of these storage types underlies the storage of regular files (e.g., &lt;code&gt;readme.txt&lt;/code&gt;) as well as relational databases and NoSQL databases.&lt;/p&gt; &lt;p&gt;But what if you need to store objects such as videos and photos that benefit from large amounts of metadata? A video might have a title, an author, a subject, tags, length, screen format, etc. Storing this metadata with the object is preferable to, say, a database entry associated with the video.&lt;/p&gt; &lt;h3&gt;Object Bucket Claims&lt;/h3&gt; &lt;p&gt;OpenShift accommodates object storage by allowing you to create an Object Bucket Claim (OBC) object. An OBC is compatible with &lt;a href="https://aws.amazon.com/s3/"&gt;AWS S3&lt;/a&gt; storage, which means that any existing object-storage code you have will likely work with OBC, needing few if any changes.&lt;/p&gt; &lt;p&gt;Using object storage means you can use an S3 client library, such as &lt;a href="https://aws.amazon.com/sdk-for-python/"&gt;Boto3&lt;/a&gt; with &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;. Metadata can be stored with an object as a collection of key-value pairs. This is the storage model of choice for items such as photos, videos, and music.&lt;/p&gt; &lt;h3&gt;The Multicloud Object Gateway&lt;/h3&gt; &lt;p&gt;But we can go deeper. The Multicloud Object Gateway (MCG) gives you more options for object storage.&lt;/p&gt; &lt;p&gt;The most powerful aspect of MCG is that it allows you to store your data across multiple cloud providers using the AWS S3 protocol. MCG is able to interact with the following types of backing stores to physically store the encrypted and deduplicated data:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://aws.amazon.com/s3/"&gt;AWS S3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.ibm.com/cloud/object-storage"&gt;IBM Cloud Object Storage (IBM COS)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://azure.microsoft.com/en-us/services/storage/blobs/"&gt;Azure Blob Storage&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://cloud.google.com/storage"&gt;Google Cloud Storage&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A local PV-based backing store&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The MCG is a layer of abstraction that separates the developer from the implementation, making data portability a real possibility. As a developer, I'm good with that.&lt;/p&gt; &lt;p&gt;MCG gives you options. As mentioned by my colleague, Michelle DiPalma, in her excellent video &lt;a href="https://www.redhat.com/en/about/videos/understanding-multicloud-object-gateway"&gt;Understanding Multicloud Object Gateway&lt;/a&gt;, MCG enables three aspects of object storage:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Flexibility across multiple backends&lt;/li&gt; &lt;li&gt;Consistent experience everywhere&lt;/li&gt; &lt;li&gt;Simplified management of siloed data&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;This is done by using three different bucket options:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;MCG object buckets (mirroring, spread across resources)&lt;/li&gt; &lt;li&gt;MCG dynamic buckets (OBC/OB inside your cluster)&lt;/li&gt; &lt;li&gt;MCG namespace buckets: data federated buckets&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;MCG object buckets ensure data safety. Your data is mirrored and spread across as many resources as you configure. You have three options for a storage architecture:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Multicloud: A mix of AWS, Azure, and GCP, for example&lt;/li&gt; &lt;li&gt;Multisite: Multiple data centers within the same cloud&lt;/li&gt; &lt;li&gt;Hybrid: Can facilitate large-scale on-premises data storage and cloud-based mirroring, for example&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Namespace buckets carry out data federation: Assembling many sources into one. Several data resources (e.g., Azure and AWS) are pulled together into one view. While the system architect decides where the data is written by an application, the location is hidden from the developers. They simply write to a named bucket; MCG takes care of writing the data and propagating it across the configured storage pool.&lt;/p&gt; &lt;h3&gt;Inside object storage&lt;/h3&gt; &lt;p&gt;OBC buckets are set up inside your cluster and are created using your application's YAML file. When you ask for an OBC, the OBC and the containing object bucket are dynamically created with permissions that you refer to in your application via environment variables. The environment variables, in a sense, bind the application and the OBC together.&lt;/p&gt; &lt;p&gt;Think of an OBC as the object storage sibling of a PVC. The OBC is persistent and is stored inside your cluster. OBC is great when you want developers to start using object storage without needing the overhead or time necessary for the final implementation (e.g., namespace buckets).&lt;/p&gt; &lt;h2&gt;Learn more about container storage&lt;/h2&gt; &lt;p&gt;This article is just an introduction to storage options for containers. There are many resources for you to dive into, learn more, and get things up and running. Here is a list to get you started:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The aforementioned video by Michelle DiPalma, &lt;a href="https://www.redhat.com/en/about/videos/understanding-multicloud-object-gateway"&gt;Understanding Multicloud Object Gateway&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage/4.8"&gt;Product Documentation for Red Hat OpenShift Container Storage 4.8&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://cloud.redhat.com/blog/introducing-multi-cloud-object-gateway-for-openshift"&gt;Introducing Multi-Cloud Object Gateway for OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift-data-foundation"&gt;Red Hat OpenShift Data Foundation&lt;/a&gt;Â (datasheet and access options)&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Writing applications for OpenShift means dealing with data. Hopefully, this article series will help you reach your goals.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/17/managing-persistent-volume-access-kubernetes" title="Managing persistent volume access in Kubernetes"&gt;Managing persistent volume access in Kubernetes&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Don Schenck</dc:creator><dc:date>2021-11-17T07:00:00Z</dc:date></entry><entry><title type="html">Auditing case management executions with Kafka and Red Hat Process Automation Manager</title><link rel="alternate" href="https://blog.kie.org/2021/11/auditing-case-management-executions-with-kafka-and-red-hat-process-automation-manager.html" /><author><name>Sadhana Nandakumar</name></author><id>https://blog.kie.org/2021/11/auditing-case-management-executions-with-kafka-and-red-hat-process-automation-manager.html</id><updated>2021-11-16T20:40:53Z</updated><content type="html">Case management provides problem resolution for dynamic processes as opposed to the efficiency-oriented approach of BPM for routine, predictable tasks. It manages one-off situations when the process flow is determined based on the incoming request. Red Hat Process Automation Manager provides the ability to define Case Management applications using the BPMN 2.0 notation. In this article, we will explore how you can capture audit metrics for a running case instance. Using Case Listeners for fine-grained and async auditing Case Events Listener can be used to capture notifications for case-related events and operations that are invoked on a case instance. This can then be sent downstream to analytical tools. The Case Events listener can be implemented by overriding any of the methods as defined by the interface.Â  In our example, we will set up a listener to capture the following events: * Case start, * Case close, * Comments added and * Case data added. We will then send them over to a Kafka topic from which this data can be visualized or analyzed. private void pushToKafka(CaseDefinition caseDefinition) { try { Future&lt;RecordMetadata&gt; out = producer.send(new ProducerRecord&lt;String, String&gt;("case_events", caseDefinition.getCaseId(), new ObjectMapper().writeValueAsString(caseDefinition))); } catch (JsonProcessingException e) { e.printStackTrace(); } } @Override public void afterCaseStarted(CaseStartEvent event) { CaseDefinition caseDefinition = new CaseDefinition(event.getCaseId(),"Case Started",null,null, new Date()); pushToKafka(caseDefinition); } @Override public void finalize() { System.out.println("case listener clean up"); producer.flush(); producer.close(); } @Override public void afterCaseDataAdded(CaseDataEvent event) { CaseDefinition caseDefinition = new CaseDefinition(event.getCaseId(),"Case Data Added",event.getData(),null, new Date()); pushToKafka(caseDefinition); }; @Override public void afterCaseDataRemoved(CaseDataEvent event) { CaseDefinition caseDefinition = new CaseDefinition(event.getCaseId(),"Case Data Removed",event.getData(),null, new Date()); pushToKafka(caseDefinition); }; @Override public void afterCaseClosed(CaseCloseEvent event) { CaseDefinition caseDefinition = new CaseDefinition(event.getCaseId(),"Case Closed",null,null, new Date()); pushToKafka(caseDefinition); }; @Override public void afterCaseCommentAdded(CaseCommentEvent event) { CaseComment caseComment = new CaseComment(event.getComment().getComment(),event.getComment().getAuthor()); CaseDefinition caseDefinition = new CaseDefinition(event.getCaseId(),"Comments Added",null,caseComment,new Date()); pushToKafka(caseDefinition); }; @Override public void afterCaseReopen(CaseReopenEvent event) { CaseDefinition caseDefinition = new CaseDefinition(event.getCaseId(),"Case Reopened",null,null, new Date()); pushToKafka(caseDefinition); } Notice how we extract the event data properties that we are interested in so that we can push it for analysis. We will then package the listener class as a maven project so that we can configure it on our case project. A complete example of the listener can be found in this . Configuring the listener on the case project: Now that we have created the listener, we will now configure it in our case project. First, we should add the listener jar to business central so that our case project can use it. You can use Business Central UI to upload the jar file. The following Aritfact upload option can be acessed through the menu CentralÂ  â Settings âArtifacts. Upload the jar file: Now, letâs add the dependency for the listener jar on our case project. You can do this by accessing, Business Central in Menu â Design â PROJECT_NAME â Settings âDependencies Next, you can configure the listener using the deployment descriptors. Access it in: Business Central in Menu â Design â PROJECT_NAME â Settings â Deployments. Finally, we can build and deploy the changes, and the listener should be able to capture case changes as they occur. Visualizing the collected data In order to visualize the data, let us set up a simple UI application. This reads from the kafka topic where we push our case metrics and shows it on a responsive UI. The application can be started using:Â  mvn quarkus:dev The UI application should be available at Testing the Case Audit Metrics: Let us create a case request. We can now see that audit metrics start populating on the UI application we created. Notice how the case start and case data added events have been captured. For every data element added to the case file, the event defines the payload associated with the data added. Similarly, other case changes like comments being added and cases being closed can be captured similarly. Summary This simple demo project for case listeners and the UI can be used with any case project. It shows how we can set up a listener for a case, and how we could push it down to Kafka for effective monitoring and audit traceability. References: The post appeared first on .</content><dc:creator>Sadhana Nandakumar</dc:creator></entry><entry><title type="html">Complex KIE server tests automation part 2: Deadline notifications after KIE server restart</title><link rel="alternate" href="https://blog.kie.org/2021/11/deadline-notifications-after-restart.html" /><author><name>Gonzalo MuÃ±oz FernÃ¡ndez</name></author><id>https://blog.kie.org/2021/11/deadline-notifications-after-restart.html</id><updated>2021-11-16T16:53:42Z</updated><content type="html">âDeadlines keep you on track for successâ (Timothy Dalton) MOTIVATION: SHOULDNâT WE AUTOMATE RESILIENCE TESTING AS WELL? This article follows the . In this ocasion, we will cover how to test deadline notifications after KIE server restart. With human task deadlines, you can set a time limit for that task, when not started or completed. Therefore, the KIE server will send a notification (by default, an automated email) to the target people defined for that task as managers, administrators, other team members. Here, we will focus on resilience testing. If the KIE server crashes, as there is a mechanism to persist the timer in a database, the KIE server will recover it after restarting. Consequently, the failover mechanism allows sending the deadline notification (triggered before crashing) at the right timing. The following figure depicts this scenario: This flow involves multiple system testing components: KIE Server, database, and SMTP server for mailing. We might be tempted to do a one-time setup instead of creating an automated test for our CI/CD pipelines. But, in the spirit of Martin Fowlerâs soundbite (reduces pain drastically), we will follow the automated approach. This way we will exercise the failover scenario regularly, taking advantage of containers.Â  First of all, we have to select our tools wisely, as the SMTP server. ONE SMTP SERVER TO TRACK THEM ALL The purpose of notification testing is to ensure that: * right inbox receives the expected emails (no more, no less). * the received emails have the right content for headers and body. So, we need a containerized SMTP server with the following requirements: * Easy to configure: minimum setup for required operations (e.g., authentication is useless to check testing emails) * Fast bootstrap: not too long to be up and running * Clear API to assist test automation: documented REST APIs to retrieve and clear emails. In our case, we have chosen ââ, an open-source SMTP server, as it fulfills these expectations and also has a docker image published in . We use the following REST APIs in our automation. Respectively, they retrieve all the messages from the inbox and delete all the messages after each test: Notice that they belong to different versions of the API (there is no DELETE operation at v2). Therefore, two different basePaths (/api/v2, /api/v1) will be set up for each endpoint in the REST-assured RequestSpecification. FITTING CONTAINERS TOGETHER Once we have defined our testing approach with self-unit independent pieces (containers) and the testing frameworks (Junit5 with and ), it is time to write our tests that will be the glue for fitting all the components together.Â  Our system-under-test -the main component- is the KIE Server. From version 7.61.0.Final, we can download it from the . From a Multistage Dockerfile, a temporary image on-the-fly will be created containing the business application (kjar) for exercising test scenarios. Therefore: * First stage, it will pull the slim maven image and install the kjars tailored for our tests. * Second stage, it will pull the KIE server image (in this case, ) with any additional configuration (jboss-cli scripts for configuring persistence and logging). The mailhog container is a testcontainers GenericContainer (that internally pulls the latest image from docker hub). It is exposing their SMTP and HTTP ports. Notice that it shares the same network as the rest of the containers: communication can occur among them without the need of exposing ports through the host. @Container public static GenericContainer&lt;?&gt; mailhog = new GenericContainer&lt;&gt;("mailhog/mailhog:latest") .withExposedPorts(PORT_SMTP, PORT_HTTP) .withNetwork(network) .withNetworkAliases("mailhog") .withLogConsumer(new Slf4jLogConsumer(logger)) .waitingFor(Wait.forHttp("/")); Finally, the PostgreSQL container is one of the out-of-the-box testcontainers database modules. We will use the initialization script under /docker-entrypoint-initdb.d containing postgresql-jbpm-schema.sql as explained . Maven build-tool pulls the schema from GitHub sources, from the same branch as the system-under-test. It is automatically downloaded using the download-maven-plugin at generate-sources phase, as it is shown in this snippet taken from the pom.xml: &lt;configuration&gt; &lt;url&gt;http://raw.githubusercontent.com/kiegroup/jbpm/${version.org.kie}/jbpm-db-scripts/src/main/resources/db/ddl-scripts/postgresql/postgresql-jbpm-schema.sql&lt;/url&gt; &lt;outputFileName&gt;postgresql-jbpm-schema.sql&lt;/outputFileName&gt; &lt;unpack&gt;false&lt;/unpack&gt; &lt;outputDirectory&gt;${project.build.directory}/postgresql&lt;/outputDirectory&gt; &lt;/configuration&gt; TESTING DEADLINE NOTIFICATIONS AFTER KIE SERVER RESTART This scenario (deadline notifications after KIE server restart) in former releases of KIE Server contained a bug tracked by :Â  duplicated emails were received in the target inboxes. The root cause was that the initialization of the human task service occurred before the deployment (upon server restart). Therefore, the timer service was not available at the moment of starting deadlines, provoking the problem of double notification. Letâs try to verify that, after fixing, it is working properly. First of all, we need a kjar containing a simple process with a human task with this deadline configuration: We can easily stop the KIE Server container after deploying the kjar and start it again to simulate a crash with a reboot.Â  And then, after deadlines timeout, we will check the expected outcome by means of REST-assured utils. We can connect to Mailhog to assure that the received emails match the expected ones: given() .spec(specV2) .when() .get("/messages") .then() .body("total", equalTo(3)) .assertThat().body("items[0].Content.Headers.To", hasItem("administrator@jbpm.org")) .assertThat().body("items[0].Content.Headers.From", hasItem("john@jbpm.org")) .assertThat().body("items[0].Content.Headers.Subject", hasItem("foo")) .assertThat().body("items[0].Content.Body", is("bar")); We also have another test for checking the case when a different kjar is deployed after the KIE server restart, then, no notification is sent because the kjar that triggered the deadline is not deployed again. Finally, you can find the code and configuration for this example . CONCLUSION: AUTOMATED TESTS FOR DEADLINE NOTIFICATIONSÂ AFTER KIE SERVER RESTART Automated Tests are also a good option to make resilience and integration testing less painful. We can combine different dockerized components (from the system-under-test -KIE server- to external databases -PostgreSQL- or SMTP servers -Mailhog-) into JUnit tests. As we control the containersâ lifecycle, it is easy to stop/start them to check failover mechanisms. The post appeared first on .</content><dc:creator>Gonzalo MuÃ±oz FernÃ¡ndez</dc:creator></entry><entry><title>Custom JFR event templates with Cryostat 2.0</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/16/custom-jfr-event-templates-cryostat-20" /><author><name>Andrew Azores, Elliott Baron</name></author><id>e48bf3e8-2a3c-4f9d-82e8-ad6ba4e344a5</id><updated>2021-11-16T07:00:00Z</updated><published>2021-11-16T07:00:00Z</published><summary type="html">&lt;p&gt;Welcome back to our series of hands-on introductions to using &lt;a href="https://developers.redhat.com/articles/2021/10/18/announcing-cryostat-20-jdk-flight-recorder-containers"&gt;Cryostat 2.0&lt;/a&gt;. This article shows you how to preconfigure custom &lt;a href="https://github.com/cryostatio/cryostat#event-templates"&gt;event templates&lt;/a&gt; for Java application monitoring with JDK Flight Recorder (JFR). First, you'll use the new &lt;a href="https://catalog.redhat.com/software/operators/detail/60ee049a744684587e218ef5"&gt;Cryostat Operator&lt;/a&gt; and a Red Hat OpenShift &lt;code&gt;ConfigMap&lt;/code&gt; to create a custom event template, then you'll instruct the Cryostat Operator to use the &lt;code&gt;ConfigMap&lt;/code&gt; when deploying Cryostat. You can use the OpenShift console to interact with Cryostat or edit the YAML file for your Cryostat custom resource. We'll demonstrate both approaches.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Cryostat is JDK Flight Recorder for &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; or &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. The &lt;a href="https://access.redhat.com/documentation/en-us/openjdk/11/html/release_notes_for_cryostat_2.0"&gt;Red Hat build of Cryostat 2.0&lt;/a&gt; is now widely available in technology preview. Cryostat 2.0 introduces many new features and improvements, such as automated rules, a better API response JSON format, custom targets, concurrent target JMX connections, WebSocket push notifications, and more. The Red Hat build includes the &lt;a href="https://catalog.redhat.com/software/operators/detail/60ee049a744684587e218ef5"&gt;Cryostat Operator&lt;/a&gt; to simplify and automate Cryostat deployment on OpenShift.&lt;/p&gt; &lt;h2&gt;Create a custom template and ConfigMap&lt;/h2&gt; &lt;p&gt;With Cryostat, you can &lt;a href="https://cryostat.io/guides/#download-edit-and-upload-a-customized-event-template"&gt;download&lt;/a&gt; existing template files directly from a Java virtual machine (JVM). Once you've downloaded a template file to your local machine, you can use the &lt;a href="https://cryostat.io/guides/#edit-template-with-jmc"&gt;JDK Mission Control&lt;/a&gt; (JMC) Template Manager to easily edit the template to suit your needs.&lt;/p&gt; &lt;p&gt;After customizing the template file, it's fairly simple to create a &lt;code&gt;ConfigMap&lt;/code&gt; from it. This &lt;code&gt;ConfigMap&lt;/code&gt; stores the template file inside of the cluster where Cryostat will run. You can use &lt;code&gt;oc&lt;/code&gt; or &lt;code&gt;kubectl&lt;/code&gt; in the namespace where Cryostat is to be deployed. Here's the source if you use the former:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; $ oc create configmap my-template --from-file=/path/to/custom.jfc &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command creates a &lt;code&gt;ConfigMap&lt;/code&gt; named &lt;code&gt;my-template&lt;/code&gt; from a file on your local machine, located at &lt;code&gt;/path/to/custom.jfc&lt;/code&gt;. The file will be placed inside the &lt;code&gt;ConfigMap&lt;/code&gt; under the &lt;code&gt;custom.jfc&lt;/code&gt; filename.&lt;/p&gt; &lt;h2&gt;Add your ConfigMap to the Cryostat Operator&lt;/h2&gt; &lt;p&gt;Once you've created a &lt;code&gt;ConfigMap&lt;/code&gt;, you only need to instruct the Cryostat Operator to use it when deploying Cryostat. You can do this when you create a Cryostat custom resource, or by updating an existing one. If you're installing Cryostat using the OpenShift console, select &lt;strong&gt;Event Templates&lt;/strong&gt; under the Cryostat custom resource that you want to update. Choose &lt;strong&gt;Add Event Template&lt;/strong&gt;. The &lt;strong&gt;Config Map Name&lt;/strong&gt; drop-down list will display all &lt;code&gt;ConfigMap&lt;/code&gt;s in the local namespace. Select the &lt;code&gt;ConfigMap&lt;/code&gt; containing the event template you just created. For &lt;strong&gt;Filename&lt;/strong&gt;, enter the name of the &lt;code&gt;.jfc&lt;/code&gt; file within the &lt;code&gt;ConfigMap&lt;/code&gt;. In Figure 1, we're using &lt;code&gt;custom.jfc&lt;/code&gt;, which we created in the previous section.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/create-cryostat-template-config-map.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/create-cryostat-template-config-map.png?itok=McevdC0z" width="600" height="236" alt="Creating an OpenShift ConfigMap containing a JDK .jfc event template definition file." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Create a ConfigMap with a JDK .jfc event template definition file. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can also work directly with the YAML for the Cryostat custom resource. The process is similar to what you did in the console. Add an &lt;code&gt;eventTemplates&lt;/code&gt; property to the &lt;code&gt;spec&lt;/code&gt; section, if one isn't already present. Then, add an array entry with &lt;code&gt;configMapName&lt;/code&gt; referencing the name of the &lt;code&gt;ConfigMap&lt;/code&gt;, and a &lt;code&gt;filename&lt;/code&gt; referencing the filename within the &lt;code&gt;ConfigMap&lt;/code&gt;. The YAML below mirrors what we did in the OpenShift console example in Figure 1.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt; apiVersion: operator.cryostat.io/v1beta1 kind: Cryostat metadata: Â Â name: cryostat-sample spec: Â Â eventTemplates:Â  Â Â - configMapName: custom-template Â Â Â Â filename: my-template.jfc &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Once you've saved your changes to the Cryostat custom resource, the Cryostat Operator will deploy Cryostat with this template preconfigured. Visiting the Cryostat web application will show that the template is present and available to use for creating new JDK flight recordings, as you can see in Figure 2.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/cryostat-template-from-config-map.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/cryostat-template-from-config-map.png?itok=FxLG5uud" width="600" height="136" alt="The Event Template list displays the template created via ConfigMap." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. The Event Template list displays the template created via ConfigMap. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, you've learned how to preconfigure Cryostat with customized event templates using OpenShift &lt;code&gt;ConfigMap&lt;/code&gt;s. You can use customized templates to gather the specific data you need in your JDK flight recordings. Visit &lt;a href="http://cryostat.io/"&gt;Cryostat.io&lt;/a&gt; and see the other articles in this series for further details:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2021/01/25/introduction-to-containerjfr-jdk-flight-recorder-for-containers"&gt;Introduction to Cryostat: JDK Flight Recorder for containers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/10/18/announcing-cryostat-20-jdk-flight-recorder-containers"&gt;Get started with Cryostat 2.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/10/26/configuring-java-applications-use-cryostat"&gt;Configuring Java applications to use Cryostat&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/02/java-monitoring-custom-targets-cryostat"&gt;Java monitoring for custom targets with Cryostat&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="ADD_URL"&gt;Automating JDK Flight Recorder in containers&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/16/custom-jfr-event-templates-cryostat-20" title="Custom JFR event templates with Cryostat 2.0"&gt;Custom JFR event templates with Cryostat 2.0&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Andrew Azores, Elliott Baron</dc:creator><dc:date>2021-11-16T07:00:00Z</dc:date></entry><entry><title>.NET 6 now available for RHEL and OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/15/net-60-now-available-rhel-and-openshift" /><author><name>Mauricio "Maltron" Leal</name></author><id>871d7b48-0893-4b7c-8e36-132292329b44</id><updated>2021-11-15T20:00:00Z</updated><published>2021-11-15T20:00:00Z</published><summary type="html">&lt;p&gt;.NET 6 is now generally available on &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux (RHEL)&lt;/a&gt; 7, RHEL 8, and &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. Here's a quick overview of what developers need to know about this new major release.&lt;/p&gt; &lt;h2&gt;New features in .NET 6&lt;/h2&gt; &lt;p&gt;In addition to x64 architecture (64-bit Intel/AMD), &lt;a href="https://developers.redhat.com/topics/dotnet/"&gt;.NET&lt;/a&gt; is now also available for ARM64 (64-bit ARM), and s390x (64-bit IBM Z) architectures.&lt;/p&gt; &lt;p&gt;.NET 6 includes new language versions &lt;a href="https://docs.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-10)"&gt;C# 10&lt;/a&gt; and &lt;a href="https://devblogs.microsoft.com/dotnet/whats-new-in-fsharp-6/"&gt;F# 6&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;ASP.NET Core adds a new &lt;a href="https://devblogs.microsoft.com/aspnet/asp-net-core-updates-in-net-6-preview-4/#introducing-minimal-apis"&gt;minimal API&lt;/a&gt; that leverages new C# 10 features to write web applications with less code.&lt;/p&gt; &lt;p&gt;Like previous versions, .NET 6 brings many &lt;a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-6/"&gt;performance improvements&lt;/a&gt; to the base libraries, GC and JIT.&lt;/p&gt; &lt;p&gt;.NET 6 introduces source generators for &lt;a href="https://devblogs.microsoft.com/dotnet/announcing-net-6-preview-4/#microsoft-extensions-logging-compile-time-source-generator"&gt;logging&lt;/a&gt; and &lt;a href="https://devblogs.microsoft.com/dotnet/try-the-new-system-text-json-source-generator/"&gt;JSON&lt;/a&gt;. Thanks to these generators, JSON serialization and logging can be performed with less allocation and better performance.&lt;/p&gt; &lt;h2&gt;How to install .NET 6&lt;/h2&gt; &lt;p&gt;You can install .NET 6 on RHEL 7 (x64 only) with the usual command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-html"&gt;# yum install rh-dotnet60&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On RHEL 8 (for x64, arm64, and s390x), enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-html"&gt;# dnf install dotnet-sdk-6.0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The .NET 6 SDK and runtime container images are available from the Red Hat Container Registry. You can use the container images as standalone images and with OpenShift on all supported architectures:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ podman run --rm registry.redhat.io/ubi8/dotnet-60 dotnet --version 6.0.100&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Long-term support for .NET 6&lt;/h2&gt; &lt;p&gt;.NET 6 is a long-term support (LTS) release. It will be supported for three years, until November 2024.&lt;/p&gt; &lt;p&gt;Based on the .NET release schedule, the next version of .NET, .NET 7, is not an LTS release. It will be released in November 2022 and supported for 18 months until May 2024.&lt;/p&gt; &lt;p&gt;The next LTS release is .NET 8, which will be released in November 2023.&lt;/p&gt; &lt;p&gt;The existing .NET Core 3.1 and .NET 5 releases will be supported until December 2022 and May 2022, respectively.&lt;/p&gt; &lt;h2&gt;Where to learn more&lt;/h2&gt; &lt;p&gt;Visit the &lt;a href="http://redhatloves.net/"&gt;.NET overview&lt;/a&gt; page to find out more about using .NET on Red Hat Enterprise Linux and OpenShift. You can also explore &lt;a href="https://developers.redhat.com/topics/dotnet"&gt;more .NET resources on Red Hat Developer&lt;/a&gt;:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Learn &lt;a href="https://developers.redhat.com/blog/2021/03/16/three-ways-to-containerize-net-applications-on-red-hat-openshift#"&gt;three ways to containerize .NET applications on Red Hat OpenShift&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/07/07/deploy-net-applications-red-hat-openshift-using-helm"&gt;Deploy .NET applications on Red Hat OpenShift using Helm&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/15/net-60-now-available-rhel-and-openshift" title=".NET 6 now available for RHEL and OpenShift"&gt;.NET 6 now available for RHEL and OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mauricio "Maltron" Leal</dc:creator><dc:date>2021-11-15T20:00:00Z</dc:date></entry></feed>
